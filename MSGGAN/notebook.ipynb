{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eac7e0e",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3715e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dataset import LoadDataset\n",
    "from MSGModel import Generator, Discriminator, DownSampleImage\n",
    "from trainMSG import train_generator, train_discriminator, evaluate\n",
    "from settings import DEVICE, MEAN, STD, BATCH_SIZE, LATENT_SIZE, EPOCHS, LOSS_FN, OPTIMIZER, BETAS, D_LR, D_LR_DECAY, G_LR, G_LR_DECAY, CORES, LOAD_MODELS\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd1d82",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fa79b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 12)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = LoadDataset('../data/train', MEAN, STD, length=8192)\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=CORES)\n",
    "num_batches = len(ds)/BATCH_SIZE\n",
    "\n",
    "val_ds = LoadDataset('../data/val', MEAN, STD)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=CORES)\n",
    "\n",
    "len(ds), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8989d",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb9be65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in Discriminator: 23137153\n",
      "Num parameters in Generator: 23053781\n"
     ]
    }
   ],
   "source": [
    "G, D, downsampler = Generator(), Discriminator(), DownSampleImage()\n",
    "# print(G)\n",
    "# print(D)\n",
    "print(f\"Num parameters in Discriminator: {sum(p.numel() for p in D.parameters())}\")\n",
    "print(f\"Num parameters in Generator: {sum(p.numel() for p in G.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c421f84",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea04b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning new model\n",
      "Start at epochs 0\n"
     ]
    }
   ],
   "source": [
    "d_losses = []\n",
    "g_losses = []\n",
    "r_scores = []\n",
    "f_scores = []\n",
    "d_optimizer = OPTIMIZER(D.parameters(), D_LR, betas=BETAS)\n",
    "g_optimizer = OPTIMIZER(G.parameters(), G_LR, betas=BETAS)\n",
    "\n",
    "try:\n",
    "    if LOAD_MODELS:\n",
    "        D.load_state_dict(torch.load('discriminator.pth'))\n",
    "        G.load_state_dict(torch.load('generator.pth'))\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        print(\"Trainning new model\")\n",
    "except:\n",
    "    print(\"Trainning new model\")\n",
    "    pass\n",
    "\n",
    "D.to(DEVICE)\n",
    "G.to(DEVICE)\n",
    "\n",
    "if not LOAD_MODELS:\n",
    "    with open(\"log.json\", \"w\") as fh:\n",
    "        json.dump({\"epochs\": 0}, fh)\n",
    "    start_epochs = 0\n",
    "else:\n",
    "    with open(\"log.json\", \"r\") as fh:\n",
    "        log = json.load(fh)\n",
    "    start_epochs = log['epochs']\n",
    "    for e in range(1, start_epochs+1):\n",
    "        d_losses.append(log[str(e)]['d_loss'])\n",
    "        g_losses.append(log[str(e)]['g_loss'])\n",
    "        r_scores.append(log[str(e)]['r_score'])\n",
    "        f_scores.append(log[str(e)]['f_score'])\n",
    "\n",
    "    d_optimizer.param_groups[0]['lr'] -= D_LR_DECAY * start_epochs\n",
    "    g_optimizer.param_groups[0]['lr'] -= G_LR_DECAY * start_epochs\n",
    "    \n",
    "    latent = torch.randn(BATCH_SIZE, *LATENT_SIZE).to(DEVICE)\n",
    "    fake_images = G(latent)[0] * 0.5 + 0.5\n",
    "    save_image(fake_images, 'current.png')\n",
    "\n",
    "print(f\"Start at epochs {start_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c6f0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b198860232874ec5a15053340224a6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epochs in range(start_epochs, EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_images in tqdm(loader):\n",
    "        batch_images = batch_images.to(DEVICE)\n",
    "        d_loss = train_discriminator(G, D, downsampler, batch_images, LOSS_FN, d_optimizer, LATENT_SIZE, BATCH_SIZE, DEVICE)\n",
    "        g_loss = train_generator(G, D, LOSS_FN, g_optimizer, LATENT_SIZE, BATCH_SIZE, DEVICE)\n",
    "        \n",
    "    d_loss, g_loss, r_score, f_score, fake_images = evaluate(G, D, downsampler, LOSS_FN, val_loader, LATENT_SIZE, BATCH_SIZE, DEVICE)\n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "    r_scores.append(r_score)\n",
    "    f_scores.append(f_score)\n",
    "    \n",
    "    torch.save(G.state_dict(), 'models/generator.pth')\n",
    "    torch.save(D.state_dict(), 'models/discriminator.pth')\n",
    "    \n",
    "    d_loss = round(d_loss,5)\n",
    "    g_loss = round(g_loss,5)\n",
    "    r_score = round(r_score,5)\n",
    "    f_score = round(f_score,5)\n",
    "\n",
    "    d_optimizer.param_groups[0]['lr'] -= D_LR_DECAY\n",
    "    g_optimizer.param_groups[0]['lr'] -= G_LR_DECAY\n",
    "\n",
    "    elapsed = round(time.time()-start, 2)\n",
    "    print(f\"Epoch #{epochs+1}: d_loss={d_loss}, g_loss={g_loss}, r_score={r_score}, f_score={f_score} elapsed={elapsed} s\")\n",
    "    \n",
    "    with open(\"log.json\", \"r\") as fh:\n",
    "        log = json.load(fh)\n",
    "    log[\"epochs\"] += 1\n",
    "    log[epochs+1] = {\"d_loss\": d_loss, \"g_loss\": g_loss, \"r_score\": r_score, \"f_score\": f_score, \"elapsed\": elapsed}\n",
    "    \n",
    "    with open(\"log.json\", \"w\") as fh:\n",
    "        json.dump(log, fh)\n",
    "    \n",
    "    save_image(fake_images, f'output/{epochs+1}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
