{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eac7e0e",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3715e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dataset import LoadDataset\n",
    "from model import Generator, Discriminator\n",
    "from train import train_generator, train_discriminator\n",
    "from settings import DEVICE, MEAN, STD, BATCH_SIZE, LATENT_SIZE, EPOCHS, LOSS_FN, OPTIMIZER, BETAS, D_LR, D_LR_DECAY, G_LR, G_LR_DECAY, CORES, LOAD_MODELS\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd1d82",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa79b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92032"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = LoadDataset('data/train', MEAN, STD)\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=CORES)\n",
    "num_batches = len(ds)/BATCH_SIZE\n",
    "\n",
    "val_ds = LoadDataset('data/val', MEAN, STD)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=CORES)\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8989d",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9be65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (upsampling): Sequential(\n",
      "    (0): ConvTranspose2d(128, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (downsampling): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (12): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (15): Conv2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (18): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (19): Sigmoid()\n",
      "    (20): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "Num parameters in Discriminator: 4890560\n",
      "Num parameters in Generator: 5410752\n"
     ]
    }
   ],
   "source": [
    "G, D = Generator(), Discriminator()\n",
    "print(G)\n",
    "print(D)\n",
    "print(f\"Num parameters in Discriminator: {sum(p.numel() for p in D.parameters())}\")\n",
    "print(f\"Num parameters in Generator: {sum(p.numel() for p in G.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c421f84",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea04b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning new model\n",
      "Start at epochs 0\n"
     ]
    }
   ],
   "source": [
    "d_losses = []\n",
    "g_losses = []\n",
    "d_optimizer = OPTIMIZER(D.parameters(), D_LR, betas=BETAS)\n",
    "g_optimizer = OPTIMIZER(G.parameters(), G_LR, betas=BETAS)\n",
    "\n",
    "try:\n",
    "    if LOAD_MODELS:\n",
    "        D.load_state_dict(torch.load('discriminator.pth'))\n",
    "        G.load_state_dict(torch.load('generator.pth'))\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        print(\"Trainning new model\")\n",
    "except:\n",
    "    print(\"Trainning new model\")\n",
    "    pass\n",
    "\n",
    "D.to(DEVICE)\n",
    "G.to(DEVICE)\n",
    "\n",
    "if not LOAD_MODELS:\n",
    "    with open(\"log.json\", \"w\") as fh:\n",
    "        json.dump({\"epochs\": 0}, fh)\n",
    "    start_epochs = 0\n",
    "else:\n",
    "    with open(\"log.json\", \"r\") as fh:\n",
    "        log = json.load(fh)\n",
    "        start_epochs = log['epochs']\n",
    "        \n",
    "    latent = torch.rand((BATCH_SIZE, *LATENT_SIZE)).to(DEVICE)\n",
    "    fake_images = G(latent)*0.5 + 0.5\n",
    "    save_image(fake_images, 'output/0.png')\n",
    "print(f\"Start at epochs {start_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de53f9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f040cc1b9cc64cdebc31402ffa8b1b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8d1564c550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jerry/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/jerry/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/jerry/miniconda3/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/jerry/miniconda3/envs/jupyter/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/jerry/miniconda3/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/jerry/miniconda3/envs/jupyter/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k8/sd7bqs553n70yr5fbdl_34jw0000gn/T/ipykernel_27368/259114383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS_FN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_LR_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLATENT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS_FN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_LR_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLATENT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WaifuGenerator/train.py\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[0;34m(G, D, images, loss_fn, optimizer, lr_decay, latent_size, batch_size, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epochs in range(start_epochs, EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_images in tqdm(loader):\n",
    "        batch_images = batch_images.to(DEVICE)\n",
    "        d_loss = train_discriminator(G, D, batch_images, LOSS_FN, d_optimizer, D_LR_DECAY, LATENT_SIZE, BATCH_SIZE, DEVICE)\n",
    "        g_loss = train_generator(G, D, LOSS_FN, g_optimizer, G_LR_DECAY, LATENT_SIZE, BATCH_SIZE, DEVICE)\n",
    "        \n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "\n",
    "    torch.save(G.state_dict(), 'generator.pth')\n",
    "    torch.save(D.state_dict(), 'discriminator.pth')\n",
    "    \n",
    "    d_loss = round(d_loss,5)\n",
    "    g_loss = round(g_loss,5)\n",
    "    r_score = round(r_score,5)\n",
    "    f_score = round(f_score,5)\n",
    "\n",
    "    elapsed = round(time.time()-start, 2)\n",
    "    print(f\"Epoch #{epochs+1}: d_loss={d_loss}, g_loss={g_loss}, r_score={r_score}, f_score={f_score} elapsed={elapsed} s\")\n",
    "    \n",
    "    with open(\"log.json\", \"r\") as fh:\n",
    "        log = json.load(fh)\n",
    "    log[\"epochs\"] += 1\n",
    "    log[epochs+1] = {\"d_loss\": d_loss, \"g_loss\": g_loss, \"r_score\": r_score, \"f_score\": f_score, \"elapsed\": elapsed}\n",
    "    \n",
    "    with open(\"log.json\", \"w\") as fh:\n",
    "        json.dump(log, fh)\n",
    "    \n",
    "    save_image(fake_images, f'output/{epochs+1}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8240b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
